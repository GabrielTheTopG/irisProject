- ## Federated
#Federated #Approaches #Distributed #Triple #Stores #Knowledge #Graphs 
Federated SPARQL processing systems [16,19,36,38,39] evaluate queries over multiple SPARQL endpoints. These systems typically target LOD and follow a query processing over data integration approach. These systems operate in a very different environment we are targeting, since we focus on exploiting distributed execution for speedup and scalabil- ity.

- ## Federated
#Federated #Approaches #Distributed #Triple #Stores #Knowledge #Graphs 
Federated queries run SPARQL queries over multiple SPARQL endpoints. A typical example is linked data, where different RDF repositories are interconnected, pro- viding a virtually integrated distributed database. Federated SPARQL query processing is a very different environment than what we target in this paper, but we discuss these sys- tems for completeness. A common technique is to precompute metadata for each individual SPARQL endpoint. Based on the metadata, the original SPARQL query is decomposed into several sub- queries, where each subquery is sent to its relevant SPARQL endpoints. The results of subqueries are then joined together to answer the original SPARQL query. In DARQ [36], the metadata are called service description that describes which triple patterns (i.e., predicate) can be answered. In [19], the metadata are called Q-Tree, which is a variant of RTree. Each leaf node in Q-Tree stores a set of source identifiers, includ- ing one for each source of a triple approximated by the node. SPLENDID [16] uses Vocabulary of Interlinked Datasets (VOID) as the metadata. HiBISCuS [38] relies on capabil- ities to compute the metadata. For each source, HiBISCuS defines a set of capabilities which map the properties to their subject and object authorities. TopFed [39] is a biological federated SPARQL query engine. Its metadata comprise of an N3 specification file and a Tissue Source Site to Tumour (TSS-to-Tumour) hash table, which is devised based on the data distribution. In contrast to these, FedX [42] does not require pre- processing, but sends “SPARQL ASK” to collect the meta- data on the fly. Based on the results of “SPARQL ASK” queries, it decomposes the query into subqueries and assign subqueries with relevant SPARQL endpoints. Global query optimization in this context has also been studied. Most federated query engines employ existing opti- mizers, such as dynamic programming [3], for optimizing the join order of local queries. Furthermore, DARQ [36] and FedX [42] discuss the use of semijoins to compute a join between intermediate results at the control site and SPARQL endpoints.

- ## Partitionbased
#Partitionbased #Approaches #Distributed #Triple #Stores #Knowledge #Graphs 
Partition-based approaches [15,18,21,22,28,29] divide the RDF graph G into a set of subgraphs (fragments) {Fi} and decompose the SPARQL query Q into subqueries {Qi}. These subqueries are then executed over the partitioned data using techniques similar to relational distributed databases.

- ## Partitionbased
#Partitionbased #Approaches #Distributed #Triple #Stores #Knowledge #Graphs 
The partition-based approaches [15,18,21,22,28,29] parti- tion an RDF graph G into several fragments and place each at a different site in a parallel/distributed system. Each site hosts a centralized RDF store of some kind. At run time, a SPARQL query Q is decomposed into several subqueries such that each subquery can be answered locally at one site, and the results are then aggregated. Each of these papers proposes its own data partitioning strategy, and different partitioning strategies result in different query processing methods. In GraphPartition [22], an RDF graph G is partitioned into n fragments, and each fragment is extended by includ- ing N-hop neighbors of boundary vertices. According to the partitioning strategy, the diameter of the graph correspond- ing to each decomposed subquery should not be larger than N to enable subquery processing at each local site. WARP 123246 P. Peng et al. [21] uses some frequent structures in workload to further extend the results of GraphPartition. Partout [15] extends the concepts of minterm predicates in relational database systems and uses the results of minterm predicates as the fragmentation units. Lee et. al. [28,29] define the partition unit as a vertex and its neighbors, which they call a “vertex block.” The vertex blocks are distributed based on a set of heuristic rules. A query is partitioned into blocks that can be executed among all sites in parallel and without any commu- nication. TriAD uses METIS [26] to divide the RDF graph into many partitions, and the number of result partitions is much more than the number of sites. Each result partition is considered as a unit and distributed among different sites. At each site, TriAD maintains six large, in-memory vectors of triples, which correspond to all SPO permutations of triples. Meanwhile, TriAD constructs a summary graph to maintain the partitioning information. All of the above methods require partitioning and dis- tributing the RDF data according to specific requirements of their approaches. However, in some applications, the RDF repository partitioning strategy is not controlled by the distributed RDF system itself. There may be some adminis- trative requirements that influence the data partitioning. For example, in some applications, the RDF knowledge bases are partitioned according to topics (i.e., different domains) or are partitioned according to different data contributors. Therefore, partition-tolerant SPARQL processing may be desirable. This is the motivation of our partial evaluation and assembly approach. Also, these approaches evaluate the SPARQL query based on query decomposition, which generate more intermediate results. We provide a detailed experimental comparison in Sect.

- ## Cloudbased
#Cloudbased #Approaches #Distributed #Triple #Stores #Knowledge #Graphs 
Cloud-based approaches (e.g., [23,27,33,34,37,48,49]) maintain a large RDF graph using existing cloud comput- ing platforms, such as Hadoop (http://hadoop.apache.org) or Cassandra (http://cassandra.apache.org), and employ triple 1 The statistic is reported in http://stats.lod2.eu/. 123244 P. Peng et al. pattern-based join processing most commonly using MapRe- duce

- ## Cloudbased
#Cloudbased #Approaches #Distributed #Triple #Stores #Knowledge #Graphs 
There have been a number of works (e.g., [23,27,33,34, 37,47–49]) focused on managing large RDF datasets using existing cloud platforms; a very good survey of these is [25]. Many of these approaches follow the MapReduce para- digm; in particular, they use HDFS [23,37,48,49], and store RDF triples in flat files in HDFS. When a SPARQL query is issued, the HDFS files are scanned to find the matches of each triple pattern, which are then joined using one of the MapReduce join implementations (see [30] for more detailed description of these). The most important difference among these approaches is how the RDF triples are stored in HDFS files; this determines how the triples are accessed and the number of MapReduce jobs. In particular, SHARD [37] directly stores the data in a single file and each line of the file represents all triples associated with a distinct subject. HadoopRDF [23] and PredicateJoin [49] further partition RDF triples based on the predicate and store each partition within one HDFS file. EAGRE [48] first groups all subjects with similar properties into an entity class and then constructs a compressed RDF graph containing only entity classes and the connections between them. It partitions the compressed RDF graph using the METIS algorithm [26]. Entities are placed into HDFS according to the partition set that they belong to. Besides the HDFS-based approaches, there are also some works that use other NoSQL distributed data stores to man- age RDF datasets. JenaHBase [27] and H2RDF [33,34] use some permutations of subject, predicate, object to build indices that are then stored in HBase (http://hbase.apache. org). Trinity.RDF [47] uses the distributed memory-cloud graph system Trinity [44] to index and store the RDF graph. It uses hashing on the vertex values to obtain a disjoint parti- tioning of the RDF graph that is placed on nodes in a cluster. These approaches benefit from the high scalability and fault tolerance offered by cloud platforms, but may suffer lower performance due to the difficulties of adapting MapRe- duce to graph computation.

